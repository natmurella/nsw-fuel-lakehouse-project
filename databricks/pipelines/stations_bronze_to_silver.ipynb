{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f48c88-1e98-4f38-b76f-000387dde91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085b371e-324e-42fd-8756-b2c4ec9cb3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, initcap, trim, from_json, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd74a9f6-91eb-41ce-85eb-1d19a50f2a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"fuel\"\n",
    "# bronze tables\n",
    "bronze_stations_table = f\"{catalog}.bronze.nsw_fuel_stations_bronze\"\n",
    "bronze_prices_table = f\"{catalog}.bronze.nsw_fuel_prices_bronze\"\n",
    "# silver tables\n",
    "silver_state_table = f\"{catalog}.silver.state\"\n",
    "silver_brand_table = f\"{catalog}.silver.brand\"\n",
    "silver_station_table = f\"{catalog}.silver.station\"\n",
    "silver_fuel_table = f\"{catalog}.silver.fuel\"\n",
    "silver_price_table = f\"{catalog}.silver.price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89e0511-77b7-44fb-85f6-111bd3a036be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_name_string_col(col_exp):\n",
    "    return initcap(lower(trim(col_exp)))\n",
    "\n",
    "def clean_acronym_string_col(col_exp):\n",
    "    return upper(trim(col_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a087231-cf89-4b74-95b8-b3452f3502ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "station_stream_df = (\n",
    "    spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(bronze_stations_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ed94649-cdb0-4a23-b7d5-76f9693eeb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver Table Batch Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4609d580-a7ed-486f-ac74-5a7d808b5d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## State Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf1b1504-2f30-4070-b9be-54ada6c3a685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_into_silver_state_table(batch_df, batch_id: int):\n",
    "\n",
    "    batch_df.createOrReplaceTempView(\"temp_batch_stations\")\n",
    "\n",
    "    # clean up and make states unique\n",
    "    cleaned_unique_states_df = spark.sql(f\"\"\"\n",
    "        select distinct upper(trim(state)) as state_code\n",
    "            from temp_batch_stations\n",
    "            where state is not null                    \n",
    "    \"\"\")\n",
    "    cleaned_unique_states_df.createOrReplaceTempView(\"temp_cleaned_states\")\n",
    "\n",
    "    # create silver state table if it doesn't exist\n",
    "    spark.sql(f\"\"\"\n",
    "        create table if not exists {silver_state_table} (\n",
    "            state_id bigint generated by default as identity,\n",
    "            state_code string not null\n",
    "        ) using delta\n",
    "    \"\"\")\n",
    "\n",
    "    # find new states not in silver state table\n",
    "    states_to_add_df = spark.sql(f\"\"\"\n",
    "        select tcs.state_code \n",
    "            from temp_cleaned_states tcs left join {silver_state_table} sss on tcs.state_code = sss.state_code\n",
    "            where sss.state_code is null\n",
    "    \"\"\")\n",
    "    states_to_add_df.createOrReplaceTempView(\"temp_states_to_add\")\n",
    "\n",
    "    # insert new states into silver state table\n",
    "    spark.sql(f\"\"\"\n",
    "        insert into {silver_state_table} (state_code)\n",
    "            select state_code\n",
    "                from temp_states_to_add\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d7b392-f456-4fa9-9d37-cc80260fe21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Brand Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2ead6c-9d03-4288-9bab-899fa12c8d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_into_silver_brand_table(batch_df, batch_id: int):\n",
    "\n",
    "    batch_df.createOrReplaceTempView(\"temp_batch_stations\")\n",
    "\n",
    "    # clean up and make brands unique\n",
    "    cleaned_unique_brands_df = spark.sql(f\"\"\"\n",
    "        select distinct initcap(lower(trim(brand))) as brand_name\n",
    "            from temp_batch_stations\n",
    "            where brand is not null                    \n",
    "    \"\"\")\n",
    "    cleaned_unique_brands_df.createOrReplaceTempView(\"temp_cleaned_brands\")\n",
    "\n",
    "    # create silver brand table if it doesn't exist\n",
    "    spark.sql(f\"\"\"\n",
    "        create table if not exists {silver_brand_table} (\n",
    "            brand_id bigint generated by default as identity,\n",
    "            brand_name string not null\n",
    "        ) using delta\n",
    "    \"\"\")\n",
    "\n",
    "    # find new brands not in silver brand table\n",
    "    brands_to_add_df = spark.sql(f\"\"\"\n",
    "        select tcb.brand_name \n",
    "            from temp_cleaned_brands tcb left join {silver_brand_table} sbs on tcb.brand_name = sbs.brand_name\n",
    "            where sbs.brand_name is null\n",
    "    \"\"\")\n",
    "    brands_to_add_df.createOrReplaceTempView(\"temp_brands_to_add\")\n",
    "\n",
    "    # insert new brands into silver brand table\n",
    "    spark.sql(f\"\"\"\n",
    "        insert into {silver_brand_table} (brand_name)\n",
    "            select brand_name\n",
    "                from temp_brands_to_add\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a90c0381-9a11-42cd-bbbf-4771241dc0d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde1c062-9e36-4178-91b5-599d58aaf8e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_into_silver_station_table(batch_df, batch_id: int):\n",
    "\n",
    "    station_df = batch_df.dropDuplicates()\n",
    "\n",
    "    # split location into latitude and longitiude\n",
    "    station_df = station_df.select(\n",
    "        \"*\", \n",
    "        col(\"location.latitude\").alias(\"latitude\"), \n",
    "        col(\"location.longitude\").alias(\"longitude\")\n",
    "    )\n",
    "    station_df = station_df.drop(\"location\")\n",
    "\n",
    "    # clean brand and state for joining\n",
    "    station_df = station_df.select(\n",
    "        \"*\",\n",
    "        clean_name_string_col(col(\"brand\")).alias(\"brand_name\"),\n",
    "        clean_acronym_string_col(col(\"state\")).alias(\"state_code\")\n",
    "    )\n",
    "\n",
    "    # replace state with state_id\n",
    "    state_df = spark.read.table(silver_state_table)\n",
    "    station_df = station_df.join(state_df, on=station_df.state_code == state_df.state_code, how=\"left\")\n",
    "    station_df = (\n",
    "        station_df.drop(\"state\")\n",
    "        .drop(\"state_code\")\n",
    "    )\n",
    "\n",
    "    # replace brand with brand_id\n",
    "    brand_df = spark.read.table(silver_brand_table)\n",
    "    station_df = station_df.join(brand_df, on=station_df.brand_name == brand_df.brand_name, how=\"left\")\n",
    "    station_df = (\n",
    "        station_df.drop(\"brand\")\n",
    "        .drop(\"brand_name\")\n",
    "    )\n",
    "\n",
    "    # drop columns\n",
    "    station_df = (\n",
    "        station_df.drop(\"_ingest_ts\")\n",
    "        .drop(\"_ingest_file\")\n",
    "        .drop(\"brandid\")\n",
    "        .drop(\"stationid\")\n",
    "    )\n",
    "\n",
    "    # rename station_code\n",
    "    station_df = station_df.withColumnRenamed(\"code\", \"station_code\")\n",
    "\n",
    "    # deduplicate station_code\n",
    "    station_df = station_df.dropDuplicates([\"station_code\"])\n",
    "\n",
    "    # create station table if it does not exist\n",
    "    spark.sql(f\"\"\"\n",
    "        create table if not exists {silver_station_table} (\n",
    "            station_id bigint generated by default as identity,\n",
    "            state_id bigint not null,\n",
    "            brand_id bigint not null,\n",
    "            station_code int not null,\n",
    "            address string,\n",
    "            name string,\n",
    "            latitude double,\n",
    "            longitude double\n",
    "        ) using delta\n",
    "    \"\"\")\n",
    "\n",
    "    # keep only new stations\n",
    "    current_stations_df = spark.sql(f\"\"\"\n",
    "        select station_code from {silver_station_table}\n",
    "    \"\"\")\n",
    "    station_df = station_df.join(current_stations_df, on=station_df.station_code == current_stations_df.station_code, how=\"leftanti\")\n",
    "\n",
    "    # insert new stations into silver station table\n",
    "    station_df.createOrReplaceTempView(\"temp_new_stations\")\n",
    "    spark.sql(f\"\"\"\n",
    "        insert into {silver_station_table} (state_id, brand_id, station_code, address, name, latitude, longitude)\n",
    "            select state_id, brand_id, station_code, address, name, latitude, longitude\n",
    "            from temp_new_stations\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bdae946-d2ef-436b-a274-0b1bbf96ef46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "680beedc-c804-4bdc-b9d8-5862c482858f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bronze_to_silver(batch_df, batch_id: int):\n",
    "\n",
    "    # process station based changes\n",
    "    ingest_into_silver_state_table(batch_df, batch_id)\n",
    "    ingest_into_silver_brand_table(batch_df, batch_id)\n",
    "    ingest_into_silver_station_table(batch_df, batch_id)\n",
    "\n",
    "query = (\n",
    "    station_stream_df.writeStream\n",
    "        .option(\"checkpointLocation\", \"/Workspace/nsw-fuel-project/silver_station_checkpoint\")\n",
    "        .trigger(availableNow=True)  # batch processinsg of all available changes\n",
    "        .foreachBatch(bronze_to_silver)\n",
    "        .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stations_bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}